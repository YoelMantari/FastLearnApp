ğŸ§  Pregunta 1

SegÃºn la definiciÃ³n de Tom Mitchell, Â¿quÃ© significa que un sistema aprenda?

A. Aumentar el nÃºmero de capas en su arquitectura
B. Mejorar su rendimiento en una tarea gracias a la experiencia (ejemplos)
C. Memorizar todos los datos de entrenamiento sin error
D. Ejecutar el algoritmo mÃ¡s rÃ¡pido que otras mÃ¡quinas

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Aprender implica optimizar parÃ¡metros para que la funciÃ³n de pÃ©rdida disminuya conforme recibe ejemplos.

ğŸ§  Pregunta 2

La neurona de McCulloch-Pitts genera una salida binaria cuando:

A. El nÃºmero de entradas es par
B. La suma ponderada de entradas sobrepasa un umbral fijo
C. Las entradas cambian de signo en tiempo real
D. Se conectan dos capas en paralelo

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Es el modelo mÃ¡s simple: suma + umbral â†’ disparo 0/1.

ğŸ§  Pregunta 3

Â¿QuiÃ©n demostrÃ³ en 1969 que el perceptrÃ³n no podÃ­a resolver problemas como XOR?

A. Yann LeCun y Yoshua Bengio
B. Marvin Minsky y Seymour Papert
C. Geoffrey Hinton y David Rumelhart
D. Alex Krizhevsky y Ilya Sutskever

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Su libro mostrÃ³ la limitaciÃ³n de linealidad, provocando el primer â€œinviernoâ€ de la IA.

ğŸ§  Pregunta 4

Los experimentos de Hubel & Wiesel inspiraron las CNN al descubrir que:

A. El cerebro procesa pÃ­xeles como vectores de 784 dimensiones
B. Existen neuronas jerÃ¡rquicas que responden primero a bordes y luego a patrones complejos
C. La corteza visual funciona exactamente como un perceptrÃ³n simple
D. Los gatos no distinguen lÃ­neas horizontales

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Mostraron una arquitectura biolÃ³gica de capas de complejidad creciente que motivÃ³ las convoluciones.

ğŸ§  Pregunta 5

El Neocognitron de Fukushima introdujo por primera vez la combinaciÃ³n de:

A. ReLU y normalizaciÃ³n de lotes
B. ConvoluciÃ³n y pooling en cascada
C. Dropout y entrenamiento en GPU
D. AtenciÃ³n y transformadores

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Fue el precursor estructural de las CNN modernas, aunque sin back-prop completo.

ğŸ§  Pregunta 6

Â¿QuÃ© problema resolviÃ³ el algoritmo de back-propagation (1986)?

A. Acelerar la convoluciÃ³n en hardware
B. Ajustar pesos en redes con mÃºltiples capas ocultas propagando el error hacia atrÃ¡s
C. Detectar bordes horizontales en tiempo real
D. Convertir imÃ¡genes a escala de grises automÃ¡ticamente

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: PermitiÃ³ entrenar redes profundas y superar la limitaciÃ³n XOR del perceptrÃ³n.

ğŸ§  Pregunta 7

La red LeNet-5 demostrÃ³ por primera vez que las CNN eran Ãºtiles para:

A. Traducir idiomas en lÃ­nea
B. Leer dÃ­gitos manuscritos de cheques bancarios a escala industrial
C. Generar imÃ¡genes sintÃ©ticas de alta resoluciÃ³n
D. Jugar ajedrez a nivel maestro

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Su arquitectura Conv-Pool repetida mÃ¡s capas densas fue un hito prÃ¡ctico en los 90.

ğŸ§  Pregunta 8

En una CNN, el tÃ©rmino receptive field se refiere a:

A. El tamaÃ±o total de la imagen de entrada
B. La porciÃ³n de la imagen que â€œveâ€ una neurona de una capa concreta
C. El nÃºmero mÃ¡ximo de parÃ¡metros entrenables
D. El tipo de funciÃ³n de activaciÃ³n utilizada

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: A medida que se apilan capas, ese campo crece y la red capta contexto mÃ¡s amplio.

ğŸ§  Pregunta 9

Â¿QuÃ© dos innovaciones fueron clave en AlexNet (2012) para reducir drÃ¡sticamente el error en ImageNet?

A. ReLU y Dropout mÃ¡s entrenamiento en GPU
B. FunciÃ³n sigmoide y perceptrÃ³n multicapa
C. Pooling promedio y normalizaciÃ³n local
D. Data augmentation y k-means de caracterÃ­sticas

âœ… Correcta: A

ğŸ§¾ ExplicaciÃ³n: ReLU acelerÃ³ el entrenamiento, Dropout mitigÃ³ sobreajuste y las GPUs hicieron viable la profundidad de la red.

ğŸ§  Pregunta 10

El estudio Zeiler & Fergus (2014) mostrÃ³ que en las CNN:

A. Todas las capas aprenden exactamente los mismos filtros
B. Las capas tempranas captan bordes, las intermedias partes de objetos y las altas prototipos completos
C. El aprendizaje ocurre solo en la Ãºltima capa densa
D. El pooling borra toda la informaciÃ³n espacial

âœ… Correcta: B

ğŸ§¾ ExplicaciÃ³n: Mediante deconvoluciÃ³n visualizaron la jerarquÃ­a progresiva de caracterÃ­sticas.